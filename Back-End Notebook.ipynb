{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Point criteria\n",
    "#64: guranteed ethnicity\n",
    "#32: 2 other possibilities\n",
    "#16: 3 other possibilities\n",
    "#8: 4 other possibilities\n",
    "#4: 5 other possibilities\n",
    "#2: 6 other possibilities\n",
    "#1: 7 other possibilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "points = {\n",
    "    'alb': 0,\n",
    "    'ang': 0,\n",
    "    'arm': 0,\n",
    "    'azr': 0,\n",
    "    'bsq': 0,\n",
    "    'bos': 0,\n",
    "    'scm': 0,\n",
    "    'bul': 0,\n",
    "    'cze': 0,\n",
    "    'dan': 0,\n",
    "    'est': 0,\n",
    "    'fin': 0,\n",
    "    'fre': 0,\n",
    "    'gae': 0,\n",
    "    'geo': 0,\n",
    "    'ger': 0,\n",
    "    'gre': 0,\n",
    "    'hun': 0,\n",
    "    'ice': 0,\n",
    "    'ita': 0,\n",
    "    'lat': 0,\n",
    "    'lit': 0,\n",
    "    'mlt': 0,\n",
    "    'mol': 0,\n",
    "    'dch': 0,\n",
    "    'pol': 0,\n",
    "    'por': 0,\n",
    "    'rom': 0,\n",
    "    'rus': 0,\n",
    "    'slk': 0,\n",
    "    'svn': 0,\n",
    "    'spa': 0,\n",
    "    'swe': 0,\n",
    "    'trk': 0,\n",
    "    'ukr': 0,\n",
    "    'wls': 0,\n",
    "}\n",
    "\n",
    "\n",
    "codes = {\n",
    "    'alb': 'Albanian',\n",
    "    'ang': 'English', #anglo\n",
    "    'arm': 'Armenian',\n",
    "    'azr': 'Azeri',\n",
    "    'bsq': 'Basque',\n",
    "    'bos': 'Bosniak',\n",
    "    'cro': 'Croatian',\n",
    "    'bul': 'Bulgarian',\n",
    "    'cze': 'Czech',\n",
    "    'dan': 'Danish', #dano-norwegian\n",
    "    'est': 'Estonian',\n",
    "    'fin': 'Finnish',\n",
    "    'fre': 'French',\n",
    "    'gae': 'Irish Gaelic', #gaelic\n",
    "    'geo': 'Georgian',\n",
    "    'ger': 'German', #germanic\n",
    "    'gre': 'Greek',\n",
    "    'hun': 'Hungarian',\n",
    "    'ice': 'Icelandic',\n",
    "    'ita': 'Italian',\n",
    "    'lat': 'Latvian',\n",
    "    'lth': 'Lithuanian',\n",
    "    'mlt': 'Maltese',\n",
    "    'mol': 'Moldovan',\n",
    "    'nor': 'Norwegian',\n",
    "    'dch': 'Dutch',\n",
    "    'pol': 'Polish',\n",
    "    'por': 'Portuguese',\n",
    "    'rom': 'Romanian',\n",
    "    'rus': 'Russian',\n",
    "    'slk': 'Slovakian',\n",
    "    'svn': 'Slovenian',\n",
    "    'spa': 'Spanish',\n",
    "    'srb': 'Serbian',\n",
    "    'swe': 'Swedish',\n",
    "    'trk': 'Turkish',\n",
    "    'ukr': 'Ukrainian',\n",
    "    'wls': 'Welsh',\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#WEB SCRAPER TO GET CHARTABLE\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "###################################################################################\n",
    "##############                       FUNCTIONS                       ##############\n",
    "\n",
    "\n",
    "#fixes broken offset on lowercase letters list, leaving a clean lowercase list\n",
    "def fixAlphabet(df):\n",
    "    letter_list = []\n",
    "\n",
    "    for column in df:\n",
    "       # print(len(column))\n",
    "        letter_list.append(column[0].lower()) #take first level and make it lowercase\n",
    "    letter_list[0] = 'Ethnicity'\n",
    "    df.columns = letter_list\n",
    "    return df\n",
    "\n",
    "#make list of desire eth indeces, then select only those\n",
    "def selectEths(df):\n",
    "    for i in df.index:\n",
    "        keep = False\n",
    "        if (df.iloc[i, 0].endswith(\"]\")):\n",
    "            df.iloc[i, 0] = df.iloc[i, 0].split('[')[0]  #remove wiki footnotes\n",
    "        if df.iloc[i, 0] == 'Romani':\n",
    "            df.iloc[i, 0] = \"drop_this\" #edge case for Romani, program was confusing romani and romanian\n",
    "        for key, value in codes.items():\n",
    "            if df.iloc[i, 0] in value:                #replace the eths with codes\n",
    "                df.iloc[i, 0] = key\n",
    "                keep = True\n",
    "        if (not keep):\n",
    "           # print(\"dropped: \" + df.iloc[i, 0])              #print when we delete to make sure we didn't accidentally delete one\n",
    "            df.iloc[i, 0] = \"drop_this\"          #replace with 'drop_this otherwise'\n",
    "    \n",
    "    dfcopy = df.copy()\n",
    "    for i in df.index:\n",
    "        if (df.iloc[i, 0] == \"drop_this\"):     #markk eths to be dropped with \"drop_this\" string\n",
    "            dfcopy.drop(i, inplace = True)\n",
    "                    \n",
    "    dfcopy.set_index('Ethnicity', inplace = True)\n",
    "\n",
    "    return dfcopy\n",
    "            \n",
    "#add missing ethnicities to the df with NaN values\n",
    "def addMissingEths(df):\n",
    "    for key in codes.keys():\n",
    "        if key not in df.index:    #if a code is not in our df yet, add it with NaN values\n",
    "            df = df.append(pd.Series(name=key, dtype='str'))        \n",
    "    return df\n",
    "\n",
    "#aggregate function to call all the methods that shape the dfs nicely\n",
    "def makeNiceDF(df):\n",
    "    df = fixAlphabet(df)\n",
    "    df = selectEths(df)\n",
    "    df = addMissingEths(df)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    " \n",
    "###################################################################################    \n",
    "##############                         MAIN                          ##############\n",
    "\n",
    "\n",
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/List_of_Latin-script_alphabets\"\n",
    "response=requests.get(wikiurl)\n",
    "\n",
    "#pull each table\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#original 26 latin letters\n",
    "latin_table = soup.find_all('table', class_=\"wikitable\")[0]  \n",
    "latin_df= pd.read_html(str(latin_table))\n",
    "latin_df = pd.DataFrame(latin_df[0])\n",
    "latin_df = makeNiceDF(latin_df)\n",
    "latin_df.drop(latin_df.columns[len(latin_df.columns)-1], axis=1, inplace=True) #drop '#' column from latin_df'\n",
    "latin_df.loc[latin_df.index[latin_df.isnull().all(1)]] = latin_df.columns #if they're all NaN, meaning they use all 26, f\n",
    "                                                                          #fill with all 26 from columns\n",
    "    \n",
    "#special letters table\n",
    "#manual adjustment due to wikitable[2] grouping langs\n",
    "special_letters_table = soup.find_all('table', class_=\"wikitable\")[2]  \n",
    "special_letters_df = pd.read_html(str(special_letters_table))\n",
    "special_letters_df = pd.DataFrame(special_letters_df[0])\n",
    "special_letters_df.iloc[7, 0] = 'Danish' #reassign Scandinavian group to just Danish\n",
    "special_letters_df.iloc[12, 0] = 'Icelandic' #reassign Icelandic/Norn to just Icelandic\n",
    "special_letters_df.iloc[13, 0] = 'French' #reassign British Isles group to just French\n",
    "special_letters_df = makeNiceDF(special_letters_df)\n",
    "\n",
    "\n",
    "#letter-diacritic combos table\n",
    "#manual adjustment due to wikitable[3] grouping langs\n",
    "letter_diacritic_table = soup.find_all('table', class_=\"wikitable\")[3]  \n",
    "letter_diacritic_df = pd.read_html(str(letter_diacritic_table))\n",
    "letter_diacritic_df = pd.DataFrame(letter_diacritic_df[0])\n",
    "letter_diacritic_df.iloc[5, 0] = 'Polish' #reassign kashubian/polish to just polish\n",
    "letter_diacritic_df.iloc[21, 0] = 'Croatian' #reassign Croatian/Sami to just Croatian\n",
    "letter_diacritic_df.iloc[28, 0] = 'Danish' #reassign Danish/Scandinavian to just Danish\n",
    "letter_diacritic_df = makeNiceDF(letter_diacritic_df)\n",
    "letter_diacritic_df.loc[['fre', 'por', 'trk'], 'ç'] = 'Ç' #assign Ç to correct langs\n",
    "letter_diacritic_df.loc['trk', 'ş'] = 'Ş'                 #assign Ş to turkish\n",
    "\n",
    "\n",
    "#a through h table\n",
    "a_h_table = soup.find_all('table', class_=\"wikitable\")[4]  \n",
    "a_h_df = pd.read_html(str(a_h_table))\n",
    "a_h_df = pd.DataFrame(a_h_df[0])\n",
    "a_h_df = makeNiceDF(a_h_df)\n",
    "#print(a_h_df)\n",
    "\n",
    "\n",
    "#i through o table\n",
    "i_o_table = soup.find_all('table', class_=\"wikitable\")[5]  \n",
    "i_o_df = pd.read_html(str(i_o_table))\n",
    "i_o_df = pd.DataFrame(i_o_df[0])\n",
    "i_o_df = makeNiceDF(i_o_df)\n",
    "#print(i_o_df)\n",
    "\n",
    "\n",
    "#p through z table\n",
    "p_z_table = soup.find_all('table', class_=\"wikitable\")[6]  \n",
    "p_z_df = pd.read_html(str(p_z_table))\n",
    "p_z_df = pd.DataFrame(p_z_df[0])\n",
    "p_z_df = makeNiceDF(p_z_df)\n",
    "#print(p_z_df)\n",
    "\n",
    "\n",
    "#combine each df into one master df with all chars\n",
    "dfs = [latin_df, special_letters_df, letter_diacritic_df, a_h_df, i_o_df, p_z_df]\n",
    "char_df = pd.concat(dfs, axis=1)\n",
    "char_df = char_df.applymap(lambda s: s.lower() if type(s) == str else s) # convert all values not Nan to lowercase\n",
    "char_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "\n",
    "#char_df.to_csv('charTable.csv')#, index=False)\n",
    "#commented so we don't recreate chartable\n",
    "\n",
    "\n",
    "df = pd.read_csv('charTable.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the .csv file, I manually removed certain chars from some languages that were excessive. Many languages had chars listed on the Wikipedia tables due to loanwords that would never occur in a native name in that language. For instance, English is listed as having Â, Ä, Û, Ü, etc. These and other symbols like them are included to facilitate integrating loanwords and foreign last names. Obviously a last name bearing one of these symbols in their name would not be of English origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL COPY OF NAMEBOT8 for testing\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "#add vowel/consonant ending code\n",
    "\n",
    "def namebot8(name):\n",
    "    #input char DF\n",
    "    char_df = pd.read_csv('charTableEdited.csv', index_col = 0)\n",
    "\n",
    "    #input suffix DF\n",
    "    suf_df = pd.read_csv('sufTable.csv', index_col = 0)\n",
    "\n",
    "    #drop eth from all tables when it fails a criterium\n",
    "    def dropEth(index):\n",
    "        char_df.drop(index=i, inplace=True)               \n",
    "        suf_df.drop(index=i, inplace=True)\n",
    "\n",
    "    name = name.lower() #make lowercase\n",
    "    name = ''.join(name.split())  #remove whitespace\n",
    "\n",
    "    c = 0 #counter for current index in name\n",
    "    for letter in name:\n",
    "        if (letter not in name[0:c]):  #skip if we already have seen this letter\n",
    "            for i in char_df.index:\n",
    "                if char_df.loc[i][letter] != letter:\n",
    "                    #print(\"dropped \" + i + \" due to: \" + letter)  #drop eth if it doesn't have letter,\n",
    "                    dropEth(i)                                    #print little report each time\n",
    "\n",
    "        c += 1\n",
    "        #return if char df len = 1\n",
    "\n",
    "\n",
    "\n",
    "    for c in suf_df.columns:\n",
    "        if name.endswith(c):\n",
    "            for i in suf_df.index:\n",
    "                if pd.isnull(suf_df.loc[i][c]):\n",
    "                    dropEth(i)\n",
    "            break\n",
    "    if len(suf_df.index) == 0:\n",
    "        return 'no possibilities'\n",
    "    return(suf_df.index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n"
     ]
    }
   ],
   "source": [
    "#TESTING ZONE\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "swe_df = pd.read_excel('1k/swe_1k.xlsx', index_col = 0)\n",
    "count = 0\n",
    "for name in swe_df.index:\n",
    "    if namebot8(name) != 'swe':               #in the xl files, the names themselves are the indeces\n",
    "        count+=1\n",
    "        print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bibliography\n",
    "https://deepbaltic.com/2016/09/23/why-you-will-almost-definitely-have-to-change-your-name-when-speaking-latvian/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
