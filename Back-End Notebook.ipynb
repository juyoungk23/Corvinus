{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Point criteria\n",
    "#64: guranteed ethnicity\n",
    "#32: 2 other possibilities\n",
    "#16: 3 other possibilities\n",
    "#8: 4 other possibilities\n",
    "#4: 5 other possibilities\n",
    "#2: 6 other possibilities\n",
    "#1: 7 other possibilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "codes = {\n",
    "    'alb': 'Albanian',\n",
    "    'ang': 'English', #anglo\n",
    "    'arm': 'Armenian',\n",
    "    'scm': 'Serbo-Croatian',\n",
    "    'dan': 'Danish', #dano-norwegian\n",
    "    'dch': 'Dutch',\n",
    "    'fin': 'Finnish',\n",
    "    'fre': 'French',\n",
    "    'geo': 'Georgian',\n",
    "    'ger': 'German', #germanic\n",
    "    'gre': 'Greek',\n",
    "    'hun': 'Hungarian',\n",
    "    'ita': 'Italian',\n",
    "    'lat': 'Latvian',\n",
    "    'lth': 'Lithuanian',\n",
    "    'pol': 'Polish',\n",
    "    'por': 'Portuguese',\n",
    "    'rom': 'Romanian',\n",
    "    'rus': 'Russian',\n",
    "    'svn': 'Slovenian',\n",
    "    'spa': 'Spanish',\n",
    "    'swe': 'Swedish',\n",
    "    'trk': 'Turkish',\n",
    "    'ukr': 'Ukrainian',\n",
    "\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'charTable.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18072/3115887685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'charTable.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'charTable.csv'"
     ]
    }
   ],
   "source": [
    "#WEB SCRAPER TO GET CHARTABLE\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "###################################################################################\n",
    "##############                       FUNCTIONS                       ##############\n",
    "\n",
    "\n",
    "#fixes broken offset on lowercase letters list, leaving a clean lowercase list\n",
    "def fixAlphabet(df):\n",
    "    letter_list = []\n",
    "\n",
    "    for column in df:\n",
    "       # print(len(column))\n",
    "        letter_list.append(column[0].lower()) #take first level and make it lowercase\n",
    "    letter_list[0] = 'Ethnicity'\n",
    "    df.columns = letter_list\n",
    "    return df\n",
    "\n",
    "#make list of desire eth indeces, then select only those\n",
    "def selectEths(df):\n",
    "    for i in df.index:\n",
    "        keep = False\n",
    "        if (df.iloc[i, 0].endswith(\"]\")):\n",
    "            df.iloc[i, 0] = df.iloc[i, 0].split('[')[0]  #remove wiki footnotes\n",
    "        if df.iloc[i, 0] == 'Romani':\n",
    "            df.iloc[i, 0] = \"drop_this\" #edge case for Romani, program was confusing romani and romanian\n",
    "        for key, value in codes.items():\n",
    "            if df.iloc[i, 0] in value:                #replace the eths with codes\n",
    "                df.iloc[i, 0] = key\n",
    "                keep = True\n",
    "        if (not keep):\n",
    "           # print(\"dropped: \" + df.iloc[i, 0])              #print when we delete to make sure we didn't accidentally delete one\n",
    "            df.iloc[i, 0] = \"drop_this\"          #replace with 'drop_this otherwise'\n",
    "    \n",
    "    dfcopy = df.copy()\n",
    "    for i in df.index:\n",
    "        if (df.iloc[i, 0] == \"drop_this\"):     #markk eths to be dropped with \"drop_this\" string\n",
    "            dfcopy.drop(i, inplace = True)\n",
    "                    \n",
    "    dfcopy.set_index('Ethnicity', inplace = True)\n",
    "\n",
    "    return dfcopy\n",
    "            \n",
    "#add missing ethnicities to the df with NaN values\n",
    "def addMissingEths(df):\n",
    "    for key in codes.keys():\n",
    "        if key not in df.index:    #if a code is not in our df yet, add it with NaN values\n",
    "            df = df.append(pd.Series(name=key, dtype='str'))        \n",
    "    return df\n",
    "\n",
    "#aggregate function to call all the methods that shape the dfs nicely\n",
    "def makeNiceDF(df):\n",
    "    df = fixAlphabet(df)\n",
    "    df = selectEths(df)\n",
    "    df = addMissingEths(df)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    " \n",
    "###################################################################################    \n",
    "##############                         MAIN                          ##############\n",
    "\n",
    "\n",
    "# get the response in the form of html\n",
    "wikiurl=\"https://en.wikipedia.org/wiki/List_of_Latin-script_alphabets\"\n",
    "response=requests.get(wikiurl)\n",
    "\n",
    "#pull each table\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#original 26 latin letters\n",
    "latin_table = soup.find_all('table', class_=\"wikitable\")[0]  \n",
    "latin_df= pd.read_html(str(latin_table))\n",
    "latin_df = pd.DataFrame(latin_df[0])\n",
    "latin_df = makeNiceDF(latin_df)\n",
    "latin_df.drop(latin_df.columns[len(latin_df.columns)-1], axis=1, inplace=True) #drop '#' column from latin_df'\n",
    "latin_df.loc[latin_df.index[latin_df.isnull().all(1)]] = latin_df.columns #if they're all NaN, meaning they use all 26, f\n",
    "                                                                          #fill with all 26 from columns\n",
    "    \n",
    "#special letters table\n",
    "#manual adjustment due to wikitable[2] grouping langs\n",
    "special_letters_table = soup.find_all('table', class_=\"wikitable\")[2]  \n",
    "special_letters_df = pd.read_html(str(special_letters_table))\n",
    "special_letters_df = pd.DataFrame(special_letters_df[0])\n",
    "special_letters_df.iloc[7, 0] = 'Danish' #reassign Scandinavian group to just Danish\n",
    "special_letters_df.iloc[12, 0] = 'Icelandic' #reassign Icelandic/Norn to just Icelandic\n",
    "special_letters_df.iloc[13, 0] = 'French' #reassign British Isles group to just French\n",
    "special_letters_df = makeNiceDF(special_letters_df)\n",
    "\n",
    "\n",
    "#letter-diacritic combos table\n",
    "#manual adjustment due to wikitable[3] grouping langs\n",
    "letter_diacritic_table = soup.find_all('table', class_=\"wikitable\")[3]  \n",
    "letter_diacritic_df = pd.read_html(str(letter_diacritic_table))\n",
    "letter_diacritic_df = pd.DataFrame(letter_diacritic_df[0])\n",
    "letter_diacritic_df.iloc[5, 0] = 'Polish' #reassign kashubian/polish to just polish\n",
    "letter_diacritic_df.iloc[21, 0] = 'Croatian' #reassign Croatian/Sami to just Croatian\n",
    "letter_diacritic_df.iloc[28, 0] = 'Danish' #reassign Danish/Scandinavian to just Danish\n",
    "letter_diacritic_df = makeNiceDF(letter_diacritic_df)\n",
    "letter_diacritic_df.loc[['fre', 'por', 'trk'], 'ç'] = 'Ç' #assign Ç to correct langs\n",
    "letter_diacritic_df.loc['trk', 'ş'] = 'Ş'                 #assign Ş to turkish\n",
    "\n",
    "\n",
    "#a through h table\n",
    "a_h_table = soup.find_all('table', class_=\"wikitable\")[4]  \n",
    "a_h_df = pd.read_html(str(a_h_table))\n",
    "a_h_df = pd.DataFrame(a_h_df[0])\n",
    "a_h_df = makeNiceDF(a_h_df)\n",
    "#print(a_h_df)\n",
    "\n",
    "\n",
    "#i through o table\n",
    "i_o_table = soup.find_all('table', class_=\"wikitable\")[5]  \n",
    "i_o_df = pd.read_html(str(i_o_table))\n",
    "i_o_df = pd.DataFrame(i_o_df[0])\n",
    "i_o_df = makeNiceDF(i_o_df)\n",
    "#print(i_o_df)\n",
    "\n",
    "\n",
    "#p through z table\n",
    "p_z_table = soup.find_all('table', class_=\"wikitable\")[6]  \n",
    "p_z_df = pd.read_html(str(p_z_table))\n",
    "p_z_df = pd.DataFrame(p_z_df[0])\n",
    "p_z_df = makeNiceDF(p_z_df)\n",
    "#print(p_z_df)\n",
    "\n",
    "\n",
    "#combine each df into one master df with all chars\n",
    "dfs = [latin_df, special_letters_df, letter_diacritic_df, a_h_df, i_o_df, p_z_df]\n",
    "char_df = pd.concat(dfs, axis=1)\n",
    "char_df = char_df.applymap(lambda s: s.lower() if type(s) == str else s) # convert all values not Nan to lowercase\n",
    "char_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "\n",
    "#char_df.to_csv('charTable.csv')#, index=False)\n",
    "#commented so we don't recreate chartable\n",
    "\n",
    "\n",
    "df = pd.read_csv('charTable.csv', index_col = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the .csv file, I manually removed certain chars from some languages that were excessive. Many languages had chars listed on the Wikipedia tables due to loanwords that would never occur in a native name in that language. For instance, English is listed as having Â, Ä, Û, Ü, etc. These and other symbols like them are included to facilitate integrating loanwords and foreign last names. Obviously a last name bearing one of these symbols in their name would not be of English origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL COPY OF NAMEBOT8 for testing\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "#add vowel/consonant ending code\n",
    "\n",
    "def namebot8(name):\n",
    "    #input char DF\n",
    "    remove_these = [3, 4, 5, 7, 10, 13, 26] #eths to remove\n",
    "\n",
    "    char_df = pd.read_csv('charTableEdited.csv', index_col = 0)\n",
    "    char_df = char_df.drop(char_df.index[remove_these])\n",
    "    #input suffix DF\n",
    "    suf_df = pd.read_csv('sufTable.csv', index_col = 0)\n",
    "    suf_df = suf_df.drop(suf_df.index[remove_these])\n",
    "    #drop eth from all tables when it fails a criterium\n",
    "    def dropEth(index):\n",
    "        char_df.drop(index=i, inplace=True)               \n",
    "        suf_df.drop(index=i, inplace=True)\n",
    "\n",
    "    name = name.lower() #make lowercase\n",
    "    name = ''.join(name.split())  #remove whitespace\n",
    "\n",
    "    c = 0 #counter for current index in name\n",
    "    for letter in name:\n",
    "        if (letter not in name[0:c]):  #skip if we already have seen this letter\n",
    "            for i in char_df.index:\n",
    "                if char_df.loc[i][letter] != letter:\n",
    "                    #print(\"dropped \" + i + \" due to: \" + letter)  #drop eth if it doesn't have letter,\n",
    "                    dropEth(i)                                    #print little report each time\n",
    "\n",
    "        c += 1\n",
    "        #return if char df len = 1\n",
    "\n",
    "\n",
    "\n",
    "    for c in suf_df.columns:\n",
    "        if name.endswith(c):\n",
    "            for i in suf_df.index:\n",
    "                if pd.isnull(suf_df.loc[i][c]):\n",
    "                    dropEth(i)\n",
    "            break\n",
    "    if len(suf_df.index) == 0:\n",
    "        return 'no possibilities'\n",
    "    return(suf_df.index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL TESTING ZONE\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd # library for data analysis\n",
    "import requests # library to handle requests\n",
    "from bs4 import BeautifulSoup # library to parse HTML documents\n",
    "\n",
    "code_arr = []\n",
    "fails_arr = []\n",
    "for key in codes.keys():\n",
    "    code_arr.append(key) \n",
    "    \n",
    "c = 0\n",
    "counter_to_100 = 0 \n",
    "for code in code_arr:\n",
    "    file_name = '1k/' + code + '_1k.xlsx'\n",
    "    cur_df = pd.read_excel(file_name, index_col = 0)   #in the xl files, the names themselves are the indeces\n",
    "    fails_arr.append(0)\n",
    "  \n",
    "    for name in cur_df.index:\n",
    "        if counter_to_100 > 99:\n",
    "            break\n",
    "        if namebot8(name) != code:\n",
    "            fails_arr[c] += 1\n",
    "        counter_to_100 +=1\n",
    "    c += 1 \n",
    "    counter_to_100 = 0\n",
    "\n",
    "#for i in range(0, len(code_arr)):\n",
    "    #print(code_arr[i], fails_arr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDIVIDUAL TESTER \n",
    "\n",
    "fails = 0    \n",
    "counter_to_100 = 0 \n",
    "\n",
    "file_name = '1k/' + 'dan' + '_1k.xlsx'\n",
    "cur_df = pd.read_excel(file_name, index_col = 0)   #in the xl files, the names themselves are the indeces\n",
    "fails_arr.append(0)\n",
    "\n",
    "for name in cur_df.index:\n",
    "    if counter_to_100 > 99:\n",
    "        break\n",
    "    if namebot8(name) != 'dan':\n",
    "        #print(name + ': ' + namebot8(name))\n",
    "        fails += 1\n",
    "\n",
    "    counter_to_100 +=1\n",
    "c += 1 \n",
    "counter_to_100 = 0\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
